{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55b9be7b-53e8-4835-8277-9821ce95cd86",
   "metadata": {
    "id": "55b9be7b-53e8-4835-8277-9821ce95cd86"
   },
   "source": [
    "## Load and Prepare CIFAR-10 Dataset\n",
    "\n",
    "Here, we load the CIFAR-10 dataset for both training and testing purposes. We use the `torchvision.datasets` module which provides a straightforward API to download and load this dataset automatically. The data is loaded into DataLoader instances that provide batches of images and corresponding labels, and allow shuffling and parallel processing using multiple worker threads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c031d24-23c2-47e0-8ae4-482ca8e0bc72",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5c031d24-23c2-47e0-8ae4-482ca8e0bc72",
    "outputId": "c027bcb3-fd2c-4e6b-9d01-4e0e4a11a181"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Set up transformations: convert images to tensors and normalize them\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Load and transform the CIFAR-10 training data\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "# Load and transform the CIFAR-10 testing data\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "# Define the classes in the CIFAR-10 dataset\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0165c23-0343-471b-b3f5-438e2eaa5637",
   "metadata": {
    "id": "f0165c23-0343-471b-b3f5-438e2eaa5637"
   },
   "source": [
    "## Define the Convolutional Neural Network\n",
    "\n",
    "This cell defines our convolutional neural network architecture using PyTorch's `nn.Module`. The network consists of two convolutional layers followed by max pooling, and three fully connected layers. This kind of architecture is common for image classification tasks. ReLU activations are used to introduce non-linearities into the model, and a pooling layer reduces the spatial dimensions of the output from convolutional layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbbe04ad-24eb-41da-9614-2e28c8efba4a",
   "metadata": {
    "id": "fbbe04ad-24eb-41da-9614-2e28c8efba4a"
   },
   "outputs": [],
   "source": [
    "# Import necessary modules for building the network\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a simple convolutional neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d666b55-c629-44f3-a112-ca43ece4116a",
   "metadata": {
    "id": "6d666b55-c629-44f3-a112-ca43ece4116a"
   },
   "source": [
    "## Setup Loss Function and Optimizer\n",
    "\n",
    "We define the loss function and the optimizer in this segment. The `CrossEntropyLoss` is suitable for multi-class classification problems like this one. We choose the SGD (Stochastic Gradient Descent) optimizer with momentum, which helps accelerate gradients vectors in the right directions, thus leading to faster converging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf00a134-19f5-40e0-a316-f939fbec408c",
   "metadata": {
    "id": "bf00a134-19f5-40e0-a316-f939fbec408c"
   },
   "outputs": [],
   "source": [
    "# Set up the loss function and optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888185eb-b640-43c6-8fb4-d16514da8fd6",
   "metadata": {
    "id": "888185eb-b640-43c6-8fb4-d16514da8fd6"
   },
   "source": [
    "## Training the Network\n",
    "\n",
    "This segment contains the training loop where the network learns from the training data. It iterates over the dataset, applies the forward and backward passes, updates the weights with the optimizer, and logs the loss periodically. This process is repeated for a specified number of epochs, allowing the model to improve its accuracy gradually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1926705-5800-4080-8d30-31b4a2f811e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1926705-5800-4080-8d30-31b4a2f811e4",
    "outputId": "8ed5e2de-e583-4afe-aa5d-e433eddf0475"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.181\n",
      "[1,  4000] loss: 1.828\n",
      "[1,  6000] loss: 1.666\n",
      "[1,  8000] loss: 1.578\n",
      "[1, 10000] loss: 1.521\n",
      "[1, 12000] loss: 1.471\n",
      "[2,  2000] loss: 1.399\n",
      "[2,  4000] loss: 1.402\n",
      "[2,  6000] loss: 1.343\n",
      "[2,  8000] loss: 1.340\n",
      "[2, 10000] loss: 1.310\n",
      "[2, 12000] loss: 1.295\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "for epoch in range(2):  # run the training loop twice\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d222d69-9efe-4a47-a2ff-299c1f7cd3bd",
   "metadata": {
    "id": "3d222d69-9efe-4a47-a2ff-299c1f7cd3bd"
   },
   "source": [
    "## Test the Network\n",
    "\n",
    "After training, we evaluate the performance of the network on the test dataset. This code calculates the total and correct predictions to determine the accuracy of the model. It's crucial to use `torch.no_grad()` during inference to indicate to PyTorch that we do not need to compute gradients, which reduces memory consumption and speeds up computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5da9a5f-7e5c-4679-862c-ca1dd84a7fca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e5da9a5f-7e5c-4679-862c-ca1dd84a7fca",
    "outputId": "225fc0ba-5579-4dc0-d7f3-d1eb4bdc649f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 52 %\n"
     ]
    }
   ],
   "source": [
    "# Test the network on the test data\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quk9PR1xtbtJ",
   "metadata": {
    "id": "quk9PR1xtbtJ"
   },
   "source": [
    "## Additional Imports and Model Enhancements\n",
    "\n",
    "In this cell, we include additional necessary imports and redefine our neural network with added complexity and regularization techniques such as dropout to prevent overfitting. We also introduce data augmentation techniques to improve the model's ability to generalize to new, unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "YwMrhLy9uQUC",
   "metadata": {
    "id": "YwMrhLy9uQUC"
   },
   "outputs": [],
   "source": [
    "# Redefine the CNN with additional complexity and dropout layers\n",
    "class EnhancedNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnhancedNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net = EnhancedNet()\n",
    "\n",
    "# Optimizer setup\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GrIrgn08ugX4",
   "metadata": {
    "id": "GrIrgn08ugX4"
   },
   "source": [
    "## Training Loop with Enhanced Network\n",
    "\n",
    "Here, we run the training loop using the enhanced model architecture. We've increased the complexity of the network and included dropout to improve the model's performance and generalization on unseen data from the CIFAR-10 dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aA1q0j0OuiMo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aA1q0j0OuiMo",
    "outputId": "0a04fc94-7c48-451e-ee60-cbf17d9105e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   500] loss: 0.498\n",
      "[1,  1000] loss: 0.511\n",
      "[1,  1500] loss: 0.494\n",
      "[1,  2000] loss: 0.506\n",
      "[1,  2500] loss: 0.527\n",
      "[1,  3000] loss: 0.468\n",
      "[1,  3500] loss: 0.516\n",
      "[1,  4000] loss: 0.509\n",
      "[1,  4500] loss: 0.499\n",
      "[1,  5000] loss: 0.516\n",
      "[1,  5500] loss: 0.526\n",
      "[1,  6000] loss: 0.512\n",
      "[1,  6500] loss: 0.529\n",
      "[1,  7000] loss: 0.512\n",
      "[1,  7500] loss: 0.542\n",
      "[1,  8000] loss: 0.525\n",
      "[1,  8500] loss: 0.521\n",
      "[1,  9000] loss: 0.539\n",
      "[1,  9500] loss: 0.524\n",
      "[1, 10000] loss: 0.525\n",
      "[1, 10500] loss: 0.525\n",
      "[1, 11000] loss: 0.528\n",
      "[1, 11500] loss: 0.538\n",
      "[1, 12000] loss: 0.535\n",
      "[1, 12500] loss: 0.518\n",
      "[2,   500] loss: 0.499\n",
      "[2,  1000] loss: 0.499\n",
      "[2,  1500] loss: 0.499\n",
      "[2,  2000] loss: 0.491\n",
      "[2,  2500] loss: 0.500\n",
      "[2,  3000] loss: 0.500\n",
      "[2,  3500] loss: 0.516\n",
      "[2,  4000] loss: 0.526\n",
      "[2,  4500] loss: 0.520\n",
      "[2,  5000] loss: 0.509\n",
      "[2,  5500] loss: 0.508\n",
      "[2,  6000] loss: 0.497\n",
      "[2,  6500] loss: 0.514\n",
      "[2,  7000] loss: 0.522\n",
      "[2,  7500] loss: 0.515\n",
      "[2,  8000] loss: 0.506\n",
      "[2,  8500] loss: 0.500\n",
      "[2,  9000] loss: 0.511\n",
      "[2,  9500] loss: 0.533\n",
      "[2, 10000] loss: 0.536\n",
      "[2, 10500] loss: 0.523\n",
      "[2, 11000] loss: 0.521\n",
      "[2, 11500] loss: 0.523\n",
      "[2, 12000] loss: 0.533\n",
      "[2, 12500] loss: 0.514\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 500 == 499:    # print every 500 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 1000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vyb_kMZi7juY",
   "metadata": {
    "id": "vyb_kMZi7juY"
   },
   "source": [
    "## Testing the Enhanced Network\n",
    "\n",
    "After training, we evaluate the performance of the enhanced network on the test dataset to see if our changes have improved accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "yb_g1MZz7bMl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yb_g1MZz7bMl",
    "outputId": "b6bfd7af-cb50-425f-b094-9ce457467b55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 61 %\n"
     ]
    }
   ],
   "source": [
    "# Test the network on the test data\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ngpm7d207b50",
   "metadata": {
    "id": "ngpm7d207b50"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
